{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's run the cell below to import all the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "from datetime import date, datetime\n",
    "import math\n",
    "import random\n",
    "import mmh3\n",
    "from datetime import datetime\n",
    "from bitarray import bitarray\n",
    "from typing import List, Tuple\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Topologies: We start by defining our three topologies: line, intersect, and multiple path.\n",
    "\n",
    "Setup a Loop: I then create a loop that iterates through a list of these topologies.\n",
    "\n",
    "Assign Variables: For each iteration, I assign the current topology to STATIONS, and based on the topology, we set LINE and GROUND_TRUTH.\n",
    "\n",
    "Process Each Topology: Inside the loop, I include the logic that processes the topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_topology = ['N_1', 'N_2', 'N_3','N_4', 'N_5']\n",
    "intersect_topology = ['N_1', 'N_5', 'N_7','N_8']\n",
    "stations_line3 = ['N_1', 'N_5', 'N_9','N_10']\n",
    "stations_line13 = ['N_1', 'N_11', 'N_12','N_13', 'N_10']\n",
    "multi_path_topology = [\n",
    "    {\"name\": \"Line 3\", \"stations\": stations_line3, \"line\": \"\\\\16003\", \"ground_truth\": \"\\\\16003\"},\n",
    "    {\"name\": \"Line13\", \"stations\": stations_line13, \"line\": \"\\\\16013\", \"ground_truth\": \"\\\\16013\"}\n",
    "]\n",
    "\n",
    "topologies = [\n",
    "    {\"name\": \"Line Topology\", \"stations\": line_topology, \"line\": \"\\\\16002\", \"ground_truth\": \"\\\\16002\"},\n",
    "    {\"name\": \"Intersect Topology\", \"stations\": intersect_topology, \"line\": \"\\\\16005\", \"ground_truth\": \"\\\\16005\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the implementation details of the Bloom Filter, including the calculation of the bit array size, the number of hash functions, and the class definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "FALSEPROB = 0.001 # False positive probability for the Bloom Filter\n",
    "\n",
    "def get_size(n, p):\n",
    "\t''' \n",
    "\tReturn the size of bit array(m) to used using\n",
    "\tfollowing formula\n",
    "\t'''\n",
    "\t#return BFLEN\n",
    "\tm = -(n * math.log(p))/(math.log(2)**2)\n",
    "\treturn int(m)\n",
    "\t\n",
    "\t\n",
    "def get_hash_count(m, n):\n",
    "\t'''\n",
    "\tReturn the hash function(k) to be used using\n",
    "\tfollowing formula\n",
    "\t'''\n",
    "\t#return NHASH\n",
    "\tk = (m/n) * math.log(2)\n",
    "\treturn int(k)\n",
    "\n",
    "class BloomFilter ( object ):\n",
    "\n",
    "\t'''\n",
    "\t\tClass for Bloom filter, using murmur3 hash function\n",
    "\t'''\n",
    "\tdef __init__(self, length, numOfHashes):\n",
    "\t\t\n",
    "\t\t'''\n",
    "\t\titems_count : int\n",
    "\t\t\tNumber of items expected to be stored in bloom filter\n",
    "\t\tfp_prob : float\n",
    "\t\t\tFalse Positive probability in decimal\n",
    "\t\t'''\n",
    "\t\t# Size of bit array to use\n",
    "\t\tself.size = length\n",
    "\t\t# number of hash functions to use\n",
    "\t\tself.hash_count = numOfHashes\n",
    "\t\t# Bit array of given size\n",
    "\t\tself.bit_array = bitarray(self.size)\n",
    "\t\t# initialize all bits as 0\n",
    "\t\tself.bit_array.setall(0)\n",
    "\t\t\n",
    "\tdef add(self, item):\n",
    "\t\t'''\n",
    "\t\tAdd an item in the filter\n",
    "\t\t'''\n",
    "\t\tdigests = []\n",
    "\t\tfor i in range(self.hash_count):\n",
    "\t\t\t# create digest for given item.\n",
    "\t\t\t# i work as seed to mmh3.hash() function\n",
    "\t\t\t# With different seed, digest created is different\n",
    "\t\t\tdigest = mmh3.hash(item, i) % self.size\n",
    "\t\t\tdigests.append(digest)\n",
    "\t\t\t# set the bit True in bit_array\n",
    "\t\t\tself.bit_array[digest] = True\n",
    "\t\treturn self.bit_array\n",
    "\n",
    "\tdef check(self, item):\n",
    "\t\t'''\n",
    "\t\tCheck for existence of an item in filter\n",
    "\t\t'''\n",
    "\t\tfor i in range(self.hash_count):\n",
    "\t\t\tdigest = mmh3.hash(item, i) % self.size\n",
    "\t\t\tif self.bit_array[digest] == False:\n",
    "\t\t\t\treturn False\n",
    "\t\treturn True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transfer_to_bfs` function is responsible for converting a dictionary of card IDs into a dictionary of Bloom filters. Each Bloom filter represents the set of card IDs associated with a specific key (e.g., station name).\n",
    "\n",
    "\n",
    "card_ids_dict: A dictionary where keys are identifiers ( station names) and values are lists of card IDs.\n",
    "\n",
    "BFLEN: The length of the Bloom filter bit array, determining its size.\n",
    "\n",
    "NHASH: The number of hash functions used in the Bloom filter, affecting its accuracy and chance of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transfer_to_bfs(card_ids_dict):\n",
    "    card_id_bfs = {}\n",
    "    for key, values in card_ids_dict.items():\n",
    "        bloomf = BloomFilter(BFLEN, NHASH)\n",
    "        for value in values:\n",
    "            bloomf.add(str(value))\n",
    "        card_id_bfs[key] = bloomf.bit_array\n",
    "\n",
    "    return card_id_bfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bitwise_or` function performs a bitwise OR operation between two bit arrays. This operation is used to combine the bits of two arrays, where the result bit is set to 1 if at least one of the bits at that position in either array is 1.\n",
    "\n",
    "[0,0,0,1,1,0,1] union [0,1,0,1,0,0,1]= [0,1,0,1,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwise_or(a, b):\n",
    "    result_or = a.copy()\n",
    "    result_or |= b\n",
    "    return result_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"bitwise_and\" function performs a bitwise AND operation between two bit arrays. This operation is used to find common elements in the sets represented by the bit arrays, where the result bit is set to 1 only if the bits at that position in both arrays are 1.\n",
    "\n",
    "Parameters\n",
    "\n",
    "a: The first bit array.\n",
    "\n",
    "b: The second bit array.\n",
    "\n",
    "Return Value\n",
    "\n",
    "Returns a new bit array resulting from the bitwise AND operation on a and b.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitwise_and(a, b):\n",
    "    result_arr = a.copy()\n",
    "    result_arr &= b\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cardinality` function estimates the number of unique elements in a set represented by a Bloom filter. This function is particularly useful for estimating the size of a set after operations like unions or intersections have been performed on the Bloom filter representing the set.\n",
    "\n",
    "The function calculates the cardinality based on the formula derived from the properties of Bloom filters. It computes the proportion of bits set to 1 (t) and uses the length of the Bloom filter (BFLEN) and the number of hash functions used (NHASH) to estimate the number of unique elements. This method relies on the assumption that the Bloom filter's bits are set in a relatively uniform distribution.\n",
    "\n",
    "Returns an integer estimate of the number of unique elements in the set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality(union_lst):\n",
    "    t = sum(int(bit) for bit in union_lst)\n",
    "    return int(-1 * (BFLEN / NHASH) * math.log(1 - t/BFLEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"calculate_accuracy\" function computes the accuracy of estimates against the ground truth. It is for evaluating the performance of our algorithms that estimate the size of tracvelers in compare with GT.\n",
    "\n",
    "Parameters\n",
    "\n",
    "gt: A list containing the ground truth values for the size of sets.\n",
    "\n",
    "estimated: A list containing the estimated values for the size of Bloom filters.\n",
    "\n",
    "Return Value\n",
    "\n",
    "Returns a list of accuracy values, calculated as 1 - (absolute difference between estimated and ground truth / ground truth), formatted to two decimal places. Perfect accuracy is represented as 1.0, and 0.0 indicates no accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(gt, estimated):\n",
    "    accuracy = []\n",
    "    for i in range(len(gt)):\n",
    "        if gt[i] == 0:\n",
    "            if estimated[i] == 0:\n",
    "                acc = 1.0  # Perfect accuracy if both are zero\n",
    "            else:\n",
    "                acc = 0.0  # No accuracy if gt is zero but estimated is not\n",
    "        else:\n",
    "            acc = max(1 - (abs(estimated[i] - gt[i]) / gt[i]), 0)\n",
    "        accuracy.append(round(acc, 2))  # Format the accuracy value to two decimal places\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_df_by_table_in_table_out` function divides the DataFrame containing route information into two separate tables: one for check-in data and another for check-out data. This separation facilitates analyses that require distinct handling of check-in and check-out events.\n",
    "\n",
    "Return Value\n",
    "\n",
    "Returns a dictionary with two keys: 'check_in' and 'check_out'. Each key maps to a DataFrame where:\n",
    "\n",
    "The check_in DataFrame contains columns for card ID, check-in time (epoch), check-in location (station), and route (line).\n",
    "\n",
    "The check_out DataFrame contains columns for card ID, check-out time (epoch), check-out location (station), and route (line).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_table_in_table_out(filtered_routes):\n",
    "    # This table selects only the columns related to check-in (card_ID, check-in time, location of check-in, and the route)\n",
    "    check_in_table = filtered_routes[['card_ID', 'check_in', 'loc1', 'Routes']]\n",
    "    check_in_table = check_in_table.rename({\"card_id\": \"card_ID\", \"loc1\": \"station\", \"check_in\": \"epoch\", 'Routes':\"line\"}, axis=1)\n",
    "    \n",
    "    # Create the check-out table\n",
    "    check_out_table = filtered_routes[['card_ID', 'check_out', 'loc2', 'Routes']]\n",
    "    check_out_table = check_out_table.rename({\"card_id\": \"card_ID\", \"loc2\": \"station\", \"check_out\": \"epoch\", 'Routes':\"line\"}, axis=1)\n",
    "    # Combine the check-in and check-out tables into a dictionary for easy access\n",
    "    dfs_in_out = {'check_in': check_in_table, 'check_out': check_out_table}    # The dictionary keys 'check_in' and 'check_out' can be used to retrieve the respective tables\n",
    "    return dfs_in_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_card_ids` function is designed to aggregate card IDs from the DataFrame based on specified station names(for each topology). This aggregation is essential for analyses that require identification of unique cards (e.g., travelers) passing through or utilizing specific stations. \n",
    "\n",
    "Returns a dictionary where each key is a station name from the STATIONS list, and the corresponding value is a list of card IDs that were recorded at that station.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_card_ids(STATIONS, filtered_df):\n",
    "    card_ids_dict = {}\n",
    "\n",
    "    for station in STATIONS:\n",
    "        # Check which columns are present in the DataFrame. This is important to determine \n",
    "        # how to filter the DataFrame based on the available information.\n",
    "        has_loc1 = 'loc1' in filtered_df.columns\n",
    "        has_loc2 = 'loc2' in filtered_df.columns\n",
    "        # If both 'loc1' and 'loc2' columns are present, it indicates a detection scenario.\n",
    "        # In this case, extract card IDs where the station is mentioned in either 'loc1' or 'loc2'.\n",
    "        if has_loc1 and has_loc2:\n",
    "            # For Detection scenario: Both loc1 and loc2 are present\n",
    "            card_ids = filtered_df[(filtered_df['loc1'] == station) | (filtered_df['loc2'] == station)]['card_ID'].tolist()\n",
    "        else:\n",
    "            # If only 'loc1' or 'loc2' is present, it indicates a scenario where we need to distinguish \n",
    "            card_ids = filtered_df[filtered_df['station'] == station]['card_ID'].tolist()\n",
    "        card_ids_dict[station] = card_ids\n",
    "\n",
    "    return card_ids_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `calculate_checkout_counts` function calculates the actual number of check-outs (ground truth) at each station within a given list of stations. This ground truth data is crucial for validating models or algorithms that estimate travelers  and station usage within each topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_checkout_counts(df, STATIONS):\n",
    "    checkout_counts = []\n",
    "    \n",
    "    for index, station in enumerate(STATIONS[1:], start=1):\n",
    "        # Get all previous stations as a list of possible check-in locations\n",
    "        possible_check_in_stations = STATIONS[:index]\n",
    "        # Filter dataframe for rows where loc2 is the current station\n",
    "        # and loc1 is in the list of possible check-in stations\n",
    "        filtered_df = df[(df['loc2'] == station) & (df['loc1'].isin(possible_check_in_stations))]\n",
    "        # Count the number of check-outs at the current station\n",
    "        checkout_count = len(filtered_df)\n",
    "        # Store the count in the dictionary with the station as the key\n",
    "        checkout_counts.append(checkout_count)\n",
    "\n",
    "    return checkout_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the first scenario, we explore the network's dynamics by considering only detection data. We use Bloom filters to represent the set of card IDs detected at each station and then calculate the estimated size of intersections between these sets. This method allows us to estimate the number of unique card IDs that were detected at a station and also at any prior stations in the sequence. \n",
    "\n",
    "(A ∪ B ∪ C ∪ D ∪ E) ∩ ( F )\n",
    "\n",
    "The \"output_IDS_bfs \"list contains the estimated counts of travelers detected up to each station, based on the union and intersections of Bloom filters.\n",
    "\n",
    "The first_result variable holds the accuracy scores of these estimates against the ground truth, providing insights into the effectiveness of using Bloom filters for this type of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_first_scenario(STATIONS: List[str], filtered_df: pd.DataFrame, BFLEN: int, GT: List[int]) -> Tuple[List[int], List[float]]:\n",
    " #Extract card IDs and transfer them to Bloom filters for each station\n",
    "    card_ids_dict = extract_card_ids(STATIONS, filtered_df)\n",
    "    card_id_bfs = transfer_to_bfs(card_ids_dict)\n",
    "\n",
    "    output_IDS_bfs = []\n",
    "    for station in reversed(STATIONS):\n",
    "        method_1 = []\n",
    "        union = bitarray([False] * BFLEN)\n",
    "        for prev_station_bfs in STATIONS[:STATIONS.index(station)]:\n",
    "            union = bitwise_or(union, card_id_bfs[prev_station_bfs])\n",
    "        inter_bfs = bitwise_and(card_id_bfs[station], union)\n",
    "        method_1.append(cardinality(inter_bfs))\n",
    "        \n",
    "        if station == STATIONS[0]:  # Stop if it's the first station\n",
    "            break\n",
    "        output_IDS_bfs.extend(method_1)\n",
    "    output_IDS_bfs.reverse()  # Reverse to maintain the correct order\n",
    "    first_result = calculate_accuracy(GT, output_IDS_bfs)\n",
    "    return output_IDS_bfs, first_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second scenario introduces a distinction between check-ins and check-outs, providing a more nuanced analysis of travelers flow. This scenario simulates a more realistic situation where passengers check in at the start of their journey and check out at their destination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_second_scenario(filtered_df, STATIONS, BFLEN, GT):\n",
    "    card_ids_dict_in = extract_card_ids(STATIONS, check_in_table)\n",
    "    card_ids_dict_out = extract_card_ids(STATIONS, check_out_table)\n",
    "    card_id_bfs_in = transfer_to_bfs(card_ids_dict_in)\n",
    "    card_id_bfs_out = transfer_to_bfs(card_ids_dict_out)\n",
    "\n",
    "    output_IDS_bfs_2 = []\n",
    "    for station in reversed(STATIONS):\n",
    "        scenario_2 = []\n",
    "        union = bitarray([False] * BFLEN)\n",
    "        for prev_station_bfs in STATIONS[:STATIONS.index(station)]:\n",
    "            union = bitwise_or(union, card_id_bfs_in[prev_station_bfs])\n",
    "        inter_bfs_2 = bitwise_and(card_id_bfs_out[station], union)\n",
    "        scenario_2.append(cardinality(inter_bfs_2))\n",
    "        \n",
    "        if station == STATIONS[0]:  # If it's the first station, stop\n",
    "            break\n",
    "        output_IDS_bfs_2.extend(scenario_2)\n",
    "    output_IDS_bfs_2.reverse()  # Reverse to maintain the correct order\n",
    "    \n",
    "    scenod_scenario_result = calculate_accuracy(GT, output_IDS_bfs_2)    \n",
    "    return output_IDS_bfs_2, scenod_scenario_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third scenario refines our analysis further by considering specific lines within the network and distinguishing between check-in and check-out activities. This approach allows for a detailed understanding of travelers flows on particular line, providing insights that are closer to real-world operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_third_scenario(check_in_table, check_out_table, STATIONS, BFLEN, LINE, GT):\n",
    "    # Filter the check-in and check-out tables for records in the selected LINE\n",
    "    check_in_table_line = check_in_table[check_in_table['line'].isin(LINE)]\n",
    "    check_out_table_line = check_out_table[check_out_table['line'].isin(LINE)]\n",
    "    # Extract card IDs and convert them to Bloom filters\n",
    "    card_ids_dict_in_line = extract_card_ids(STATIONS, check_in_table_line)\n",
    "    card_ids_dict_out_line = extract_card_ids(STATIONS, check_out_table_line)\n",
    "    card_id_bfs_in_line = transfer_to_bfs(card_ids_dict_in_line)\n",
    "    card_id_bfs_out_line = transfer_to_bfs(card_ids_dict_out_line)\n",
    "\n",
    "    output_IDS_bfs_3 = []\n",
    "    for station in reversed(STATIONS):\n",
    "        scenario_3 = []\n",
    "        union = bitarray([False] * BFLEN)\n",
    "        for prev_station_bfs in STATIONS[:STATIONS.index(station)]:\n",
    "            union = bitwise_or(union, card_id_bfs_in_line[prev_station_bfs])\n",
    "        inter_bfs_3 = bitwise_and(card_id_bfs_out_line[station], union)\n",
    "        scenario_3.append(cardinality(inter_bfs_3))\n",
    "        \n",
    "        if station == STATIONS[0]:  # Stop if it's the first station\n",
    "            break\n",
    "        output_IDS_bfs_3.extend(scenario_3)\n",
    "    output_IDS_bfs_3.reverse()  # Reverse to maintain the correct order\n",
    "    third_scenario_result = calculate_accuracy(GT, output_IDS_bfs_3)    \n",
    "    return output_IDS_bfs_3, third_scenario_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: calculate_scenario_accuracy\n",
    "\n",
    "The calculate_scenario_accuracy function is designed to compute the cardinality and accuracy of a given scenario \"within a multi-path network topolog\"y analysis. It utilizes Bloom filters to estimate the size of intersections and evaluates the accuracy based on a provided ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scenario_accuracy(card_id_bfs_4, boeierbrug_value, GT_line3_13, BFLEN):\n",
    "    union = bitarray([False] * BFLEN)\n",
    "    for station in card_id_bfs_4:\n",
    "        union = bitwise_or(union, card_id_bfs_4[station])\n",
    "    inter_bfs_line3_line13_scenario1 = bitwise_and(boeierbrug_value, union)\n",
    "    scenario1_cardinality = [cardinality(inter_bfs_line3_line13_scenario1)]\n",
    "    multi_paths_scenario_1_acc = calculate_accuracy(GT_line3_13, scenario1_cardinality)\n",
    "\n",
    "    return scenario1_cardinality, multi_paths_scenario_1_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we load our dataset and prepare it for analysis by assigning unique identifiers to each traveler. This process is crucial for anonymizing the data and ensuring that each record can be individually tracked through the transportation network without revealing personal information.\n",
    "\n",
    "\n",
    "We start by loading the dataset from a CSV file. This dataset, presumably named `lelystad_data.csv`, contains records of travelers' movements within the Lelystad transportation network. The data is separated by semicolons (`;`), a common format for datasets where commas may appear within individual data fields.\n",
    "\n",
    "Before employing Bloom Filters in our analysis, it's necessary to initialize their parameters. These parameters are crucial for optimizing the filter's efficiency and accuracy in representing the set of items (in our case card IDs) without taking up too much space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "                   # Main\n",
    "########################################################\n",
    "df = pd.read_csv('synthetic_topology_data.csv', sep=';')\n",
    "travelerIDSet = random.sample(range(100000000000000), len(df)) # Generate random traveler IDs\n",
    "df['card_ID']=travelerIDSet # Assign random IDs to 'card_ID' column\n",
    "BFLEN = get_size(len(df), FALSEPROB)\n",
    "NHASH = get_hash_count(BFLEN, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1- The code snippet below demonstrates how to iterate through a list of topologies, each represented as a dictionary with keys for the topology's name, the stations involved, the associated line, and the ground truth data. For each topology, the DataFrame is filtered to include only rows where the location of the event (`loc1` for check-ins or `loc2` for check-outs) matches one of the stations in the topology.\n",
    "\n",
    "2- To evaluate the effectiveness of our analysis, it's crucial to establish a ground truth against which we can compare our estimates. The ground truth in this context refers to the actual number of check-outs at each station, which we calculate based on the filtered dataset. \n",
    "\n",
    "3- The filter_line DataFrame is a subset of the original data, filtered to include only transactions that occurred on the routes defined by LINE for each topology.\n",
    "\n",
    "4- GT is a list containing the actual number of check-outs at each station within the STATIONS list, providing a basis for assessing the accuracy of our later analyses.\n",
    "\n",
    "5- First scenario results\n",
    "\n",
    "6- The second scenario results.\n",
    "\n",
    "7- The third scenario results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running analysis for Line Topology\n",
      "GT: [145, 247, 355, 529]\n",
      "First Scenario Estimated Sizes: [421, 645, 816, 2181]\n",
      "First Scenario Accuracies: [0, 0, 0, 0]\n",
      "Second Scenario Estimated Sizes: [190, 285, 375, 1059]\n",
      "Second scenario Accuracies: [0.69, 0.85, 0.94, 0]\n",
      "Third Scenario Estimated Sizes: [152, 258, 363, 528]\n",
      "Third scenario Accuracies: [0.95, 0.96, 0.98, 1.0]\n",
      "\n",
      "Running analysis for Intersect Topology\n",
      "GT: [194, 401, 613]\n",
      "First Scenario Estimated Sizes: [1480, 980, 1244]\n",
      "First Scenario Accuracies: [0, 0, 0]\n",
      "Second Scenario Estimated Sizes: [626, 439, 614]\n",
      "Second scenario Accuracies: [0, 0.91, 1.0]\n",
      "Third Scenario Estimated Sizes: [202, 412, 614]\n",
      "Third scenario Accuracies: [0.96, 0.97, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Filter DataFrame for rows where either 'loc1' or 'loc2' matches the stations in the selected topology\n",
    "for topology in topologies:\n",
    "    STATIONS = topology[\"stations\"]\n",
    "    LINE = [topology[\"line\"]]\n",
    "    GROUND_TRUTH = [topology[\"ground_truth\"]]\n",
    "    \n",
    "    print(f\"\\nRunning analysis for {topology['name']}\")\n",
    " \n",
    "    filtered_df = df[df['loc1'].isin(STATIONS) | df['loc2'].isin(STATIONS)] \n",
    "\n",
    "    ########################### Calculate ground truth (GT) for the checkout counts at each station###################################\n",
    "    filter_line = filtered_df[filtered_df['Routes'].isin(LINE)]\n",
    "    GT = calculate_checkout_counts(filter_line, STATIONS)\n",
    "    print(\"GT:\",GT)\n",
    "    tables_dict = split_df_by_table_in_table_out(filtered_df)\n",
    "    check_in_table = tables_dict['check_in']\n",
    "    check_out_table = tables_dict['check_out']\n",
    "    \n",
    "    # first scenario\n",
    "    first_scenario_estimated_sizes, first_cenario_accuracies = calculate_first_scenario(STATIONS, filtered_df, BFLEN, GT)\n",
    "    print(\"First Scenario Estimated Sizes:\", first_scenario_estimated_sizes)\n",
    "    print(\"First Scenario Accuracies:\", first_cenario_accuracies)\n",
    "    \n",
    "    # second scenario\n",
    "    estimated_sizes_second_scenario, accuracies_second_scenario = calculate_second_scenario(filtered_df, STATIONS, BFLEN, GT)\n",
    "    print(\"Second Scenario Estimated Sizes:\", estimated_sizes_second_scenario)\n",
    "    print(\"Second scenario Accuracies:\", accuracies_second_scenario)\n",
    "    \n",
    "    # third scenario\n",
    "    estimated_sizes_third_scenario, accuracies_third_scenario = calculate_third_scenario(check_in_table, check_out_table, STATIONS, BFLEN, LINE, GT)\n",
    "    print(\"Third Scenario Estimated Sizes:\", estimated_sizes_third_scenario)\n",
    "    print(\"Third scenario Accuracies:\", accuracies_third_scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running analysis for multi-path topology involves calculating the ground truth values for checkout counts at each station, given a set of paths. Each path includes stations and a specific line identifier. The process is as follows:\n",
    "\n",
    "Iterate over each path in multi_path_topology.\n",
    "For each path, identify the stations and line involved.\n",
    "Filter the DataFrame to include only the relevant stations for the current path.\n",
    "Calculate the ground truth (GT) for the checkout counts at each station.\n",
    "Sum the last ground truth values from both paths to obtain a combined ground truth for the multi-path scenario as the Destination or inetersted node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running analysis for multi_path_topology\n",
      "Combined GT: [1144]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRunning analysis for multi_path_topology\")\n",
    "# Ground truth\n",
    "gt_values = [] \n",
    "for path in multi_path_topology:\n",
    "    STATIONS = path[\"stations\"]\n",
    "    LINE = [path[\"line\"]]\n",
    "    GROUND_TRUTH = [path[\"ground_truth\"]]\n",
    "    filtered_df = df[df['loc1'].isin(STATIONS) | df['loc2'].isin(STATIONS)] # looking at nodes we are interested not the whole nodes.\n",
    "    ########################### Calculate ground truth (GT) for the checkout counts at each station###################################\n",
    "    filter_line = filtered_df[filtered_df['Routes'].isin(LINE)]\n",
    "    GT = calculate_checkout_counts(filter_line, STATIONS)\n",
    "    gt_values.append(GT[-1])   \n",
    "GT_line3_13 = [sum(gt_values)]  # Summing the last GT values from both paths\n",
    "print(\"Combined GT:\", GT_line3_13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario 1 Analysis\n",
    "\n",
    "In the first scenario, we aim to analyze the network topology by combining stations from two different lines, stations_line3 and stations_line13, to create a unified list of stations. This list is then used to filter our dataset for relevant records. The goal is to calculate the estimated sizes and accuracies of the multi-path topology for these combined stations, focusing on a specific station of interest: 'Lls Boeierbrug' as destination node. \n",
    "\n",
    "The steps involved are:\n",
    "\n",
    "Merge stations_line3 and stations_line13 lists to form a unified set of unique stations.\n",
    "\n",
    "Filter the DataFrame to include only records related to these stations.\n",
    "\n",
    "Extract and transform these stations into Bloom filters.\n",
    "\n",
    "Remove the Bloom filter for 'Lls Boeierbrug' to use it in a separate intersection analysis.\n",
    "\n",
    "Create a union of all remaining station Bloom filters.\n",
    "\n",
    "Calculate the cardinality of the intersection between this union and the 'Lls Boeierbrug' filter.\n",
    "\n",
    "Compute the accuracy of this scenario by comparing the cardinality against the combined ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Scenario Estimated Sizes Multi paths: [1044]\n",
      "First scenario Accuracies Multi paths: [0.91]\n"
     ]
    }
   ],
   "source": [
    "#scenario1\n",
    "STATIONS  = list(set(stations_line3 + stations_line13))\n",
    "filtered_df = df[df['loc1'].isin(STATIONS) | df['loc2'].isin(STATIONS)] # looking at nodes we are interested not the whole nodes.\n",
    "card_ids_dict_4 = extract_card_ids(STATIONS, filtered_df)\n",
    "card_id_bfs_4 = transfer_to_bfs(card_ids_dict_4)\n",
    "boeierbrug_value = card_id_bfs_4.pop('N_11', None)\n",
    "union = bitarray([False] * BFLEN)\n",
    "scenario1_cardinality, multi_paths_scenario_1_acc = calculate_scenario_accuracy(card_id_bfs_4, boeierbrug_value, GT_line3_13,BFLEN)\n",
    "print(\"First Scenario Estimated Sizes Multi paths:\", scenario1_cardinality)\n",
    "print(\"First scenario Accuracies Multi paths:\", multi_paths_scenario_1_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario 2 Analysis\n",
    "\n",
    "Scenario 2 extends the analysis by distinguishing between check-in and check-out activities at the stations, considering the same unified list of stations from Scenario 1. The objective is to understand the interactions at these stations with an emphasis on the 'Lls Boeierbrug' station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second Scenario Estimated Sizes Multi paths: [511]\n",
      "second scenario Accuracies Multi paths: [0.45]\n"
     ]
    }
   ],
   "source": [
    "#scenario2\n",
    "tables_dict_3_13 = split_df_by_table_in_table_out(filtered_df)\n",
    "check_in_table_3_13 = tables_dict_3_13['check_in']\n",
    "check_out_table_3_13 = tables_dict_3_13['check_out']\n",
    "card_ids_dict_in_s2 = extract_card_ids(STATIONS, check_in_table_3_13)\n",
    "card_ids_dict_out_s2 = extract_card_ids(STATIONS, check_out_table_3_13)\n",
    "card_id_bfs_in_s2 = transfer_to_bfs(card_ids_dict_in_s2)\n",
    "card_id_bfs_out_s2 = transfer_to_bfs(card_ids_dict_out_s2)\n",
    "boeierbrug_value_s2 = card_id_bfs_out_s2.pop('N_11', None)\n",
    "card_id_bfs_in_s2.pop('Lls Boeierbrug', None)\n",
    "scenario2_cardinality, multi_paths_scenario_2_acc = calculate_scenario_accuracy(card_id_bfs_in_s2, boeierbrug_value_s2, GT_line3_13,BFLEN)\n",
    "print(\"second Scenario Estimated Sizes Multi paths:\", scenario2_cardinality)\n",
    "print(\"second scenario Accuracies Multi paths:\", multi_paths_scenario_2_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scenario 3, our focus shifts to a detailed examination of two specific lines: Line 03 and Line 13. This scenario aims to delve deeper into the check-in and check-out dynamics at the stations belonging to these lines, with a particular emphasis on understanding the role of the 'Lls Boeierbrug' station.\n",
    "\n",
    "The analytical steps for this scenario include:\n",
    "\n",
    "Filtering the check-in and check-out data tables to retain only those records associated with the targeted lines, Line 03 and Line 13.\n",
    "\n",
    "From these filtered tables, extracting card IDs and transforming them into Bloom filters for a more refined analysis.\n",
    "\n",
    "Isolating the 'Lls Boeierbrug' station's Bloom filter from both the check-in and check-out sets to allow for an individualized analysis of its interaction patterns.\n",
    "\n",
    "Calculating the union of all \"in\" station Bloom filters, excluding 'Lls Boeierbrug', to gauge the cumulative interactions across these stations.\n",
    "\n",
    "Conducting a bitwise AND operation between the 'Lls Boeierbrug' filter (from the check-out set) and the union of \"in\" Bloom filters to estimate the interaction scale.\n",
    "\n",
    "Determining the cardinality from this interaction to estimate the scenario's size.\n",
    "\n",
    "Comparing the estimated size with the combined ground truth for Lines 03 and 13 to measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third Scenario Estimated Sizes Multi paths: [511]\n",
      "Third scenario Accuracies Multi paths: [0.45]\n"
     ]
    }
   ],
   "source": [
    "#Third scenario\n",
    "LINE = [\"\\\\16003\", \"\\\\16013\"]\n",
    "check_in_table_line = check_in_table_3_13[check_in_table_3_13['line'].isin(LINE)]\n",
    "check_out_table_line = check_out_table_3_13[check_out_table_3_13['line'].isin(LINE)]\n",
    "card_ids_dict_in_s3 = extract_card_ids(STATIONS, check_in_table_line)\n",
    "card_ids_dict_out_s3 = extract_card_ids(STATIONS, check_out_table_line)\n",
    "card_id_bfs_in_s3 = transfer_to_bfs(card_ids_dict_in_s3)\n",
    "card_id_bfs_out_s3 = transfer_to_bfs(card_ids_dict_out_s3)\n",
    "boeierbrug_value_s3 = card_id_bfs_out_s3.pop('N_11', None)\n",
    "card_id_bfs_in_s3.pop('N_11', None)\n",
    "scenario3_cardinality, multi_paths_scenario_3_acc = calculate_scenario_accuracy(card_id_bfs_in_s3, boeierbrug_value_s3, GT_line3_13,BFLEN)\n",
    "print(\"Third Scenario Estimated Sizes Multi paths:\", scenario3_cardinality)\n",
    "print(\"Third scenario Accuracies Multi paths:\", multi_paths_scenario_3_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
